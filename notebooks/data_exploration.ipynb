{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf58ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebooks/data_exploration.py\n",
    "# ---\n",
    "# jupyter:\n",
    "#   jupytext:\n",
    "#     formats: ipynb,py\n",
    "#     text_representation:\n",
    "#       extension: .py\n",
    "#       format_name: light\n",
    "# ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b738f540",
   "metadata": {},
   "source": [
    "# LoL Draft Predictor — Data Exploration\n",
    "This notebook rebuilds the DuckDB schema, peeks at each feature table,\n",
    "and runs quick train+eval for all iterations so you can compare side-by-side."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72570a07",
   "metadata": {},
   "source": [
    "## 1) Rebuild all SQL tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9886d65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "# Point this at your duckdb file\n",
    "DB=\"data/lol.duckdb\"\n",
    "\n",
    "for f in [\n",
    "    \"scripts/sql/schema_raw.sql\",\n",
    "    \"scripts/sql/create_team_stats.sql\",\n",
    "    \"scripts/sql/create_match_stats.sql\",\n",
    "    \"scripts/sql/create_champion_meta.sql\",\n",
    "    \"scripts/sql/create_champion_synergy.sql\",\n",
    "    \"scripts/sql/create_champion_counters.sql\",\n",
    "    \"scripts/sql/create_player_champion_stats.sql\",\n",
    "    \"scripts/sql/create_team_performance.sql\",\n",
    "    \"scripts/sql/create_team_form.sql\"\n",
    "]:\n",
    "    print(f\"▶ running {f}\")\n",
    "    subprocess.run(f\"duckdb {DB} < {f}\", shell=True, check=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0670142",
   "metadata": {},
   "source": [
    "## 2) Peek at feature tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9003b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb, pandas as pd\n",
    "\n",
    "con = duckdb.connect(DB)\n",
    "\n",
    "tables = [\n",
    "    \"team_picks\", \"match_picks\", \n",
    "    \"champion_meta\",\"champion_synergy\",\"champion_counters\",\n",
    "    \"player_champion_stats\",\"team_performance\",\"team_form\"\n",
    "]\n",
    "\n",
    "for t in tables:\n",
    "    df = con.execute(f\"SELECT * FROM {t} LIMIT 5\").df()\n",
    "    print(f\"\\n### {t} (5 rows)\")\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36b0c2e",
   "metadata": {},
   "source": [
    "## 3) Run training & evaluation for each iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a2d8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, joblib\n",
    "import numpy as np\n",
    "from lolpredictor.etl import get_connection, load_mvp_features\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, log_loss\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from scripts.train import train_and_save  # noqa: E402\n",
    "from scripts.evaluate import main as evaluate_main  # noqa: E402\n",
    "import config\n",
    "\n",
    "# ensure models dir exists\n",
    "os.makedirs(config.MODELS_DIR, exist_ok=True)\n",
    "\n",
    "results = []\n",
    "for iteration in [\"baseline\",\"meta\",\"counters\",\"profiles\",\"form\"]:\n",
    "    out = os.path.join(\n",
    "        config.MODELS_DIR,\n",
    "        f\"{iteration}_{pd.Timestamp.now().strftime('%Y%m%d')}.pkl\"\n",
    "    )\n",
    "    print(f\"\\n## Training: {iteration}\")\n",
    "    train_and_save(iteration, config.DB_PATH, out)\n",
    "\n",
    "    print(f\"\\n## Evaluating: {iteration}\")\n",
    "    # capture stdout of evaluate.py by monkey‐patching args\n",
    "    import sys\n",
    "    sys.argv = [\n",
    "        \"evaluate.py\",\n",
    "        \"--model\", out,\n",
    "        \"--db\", config.DB_PATH,\n",
    "        \"--split-date\", config.SPLIT_DATE\n",
    "    ]\n",
    "    evaluate_main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f262e3df",
   "metadata": {},
   "source": [
    "## 4) Summary: manually inspect the printed metrics above,\n",
    "or you can wrap the evaluation outputs into a DataFrame for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f09f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) code here to parse the printed logs into a DataFrame"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
